{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb34ed4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:41.133224Z",
     "iopub.status.busy": "2025-11-25T16:26:41.133021Z",
     "iopub.status.idle": "2025-11-25T16:26:41.656548Z",
     "shell.execute_reply": "2025-11-25T16:26:41.655854Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.529479,
     "end_time": "2025-11-25T16:26:41.657821",
     "exception": false,
     "start_time": "2025-11-25T16:26:41.128342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input:\r\n",
      "create-dataset-for-projectnlp\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp:\r\n",
      "custom.css  data  __notebook__.ipynb  __output__.json  __results__.html\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data:\r\n",
      "PhoMT_100000   PhoMT_200000  PhoMT_400000  PhoMT_600000  PhoMT_800000\r\n",
      "PhoMT_1000000  PhoMT_300000  PhoMT_500000  PhoMT_700000  PhoMT_900000\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_100000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_100000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_100000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_100000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_100000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_1000000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_1000000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_1000000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_1000000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_1000000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_200000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_200000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_200000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_200000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_200000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_300000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_300000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_300000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_300000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_300000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_400000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_400000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_400000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_400000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_400000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_500000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_500000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_500000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_500000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_500000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_600000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_600000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_600000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_600000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_600000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_800000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_800000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_800000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_800000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_800000/tokenization/train:\r\n",
      "train.en  train.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_900000:\r\n",
      "tokenization\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_900000/tokenization:\r\n",
      "dev  test  train\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_900000/tokenization/dev:\r\n",
      "dev.en\tdev.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_900000/tokenization/test:\r\n",
      "test.en  test.vi\r\n",
      "\r\n",
      "/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_900000/tokenization/train:\r\n",
      "train.en  train.vi\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R /kaggle/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ed308d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:41.666121Z",
     "iopub.status.busy": "2025-11-25T16:26:41.665887Z",
     "iopub.status.idle": "2025-11-25T16:26:42.112473Z",
     "shell.execute_reply": "2025-11-25T16:26:42.111721Z"
    },
    "papermill": {
     "duration": 0.452255,
     "end_time": "2025-11-25T16:26:42.113918",
     "exception": false,
     "start_time": "2025-11-25T16:26:41.661663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!mkdir -p data\n",
    "!mkdir -p data/sp\n",
    "!mkdir -p data/src\n",
    "!mkdir -p data/trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2247e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:42.122191Z",
     "iopub.status.busy": "2025-11-25T16:26:42.121957Z",
     "iopub.status.idle": "2025-11-25T16:26:46.117053Z",
     "shell.execute_reply": "2025-11-25T16:26:46.116307Z"
    },
    "papermill": {
     "duration": 4.000601,
     "end_time": "2025-11-25T16:26:46.118217",
     "exception": false,
     "start_time": "2025-11-25T16:26:42.117616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/data/raw_data.src\n",
      "/kaggle/working/data/raw_data.trg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "base_path = \"/kaggle/input/create-dataset-for-projectnlp/data/PhoMT_700000/tokenization\"\n",
    "\n",
    "# Hàm đọc file\n",
    "def read_file(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Đọc train\n",
    "train_en = read_file(f\"{base_path}/train/train.en\")\n",
    "train_vi = read_file(f\"{base_path}/train/train.vi\")\n",
    "\n",
    "# Đọc dev\n",
    "dev_en = read_file(f\"{base_path}/dev/dev.en\")\n",
    "dev_vi = read_file(f\"{base_path}/dev/dev.vi\")\n",
    "\n",
    "# Đọc test\n",
    "test_en = read_file(f\"{base_path}/test/test.en\")\n",
    "test_vi = read_file(f\"{base_path}/test/test.vi\")\n",
    "\n",
    "# Gộp lại\n",
    "all_src = train_en + dev_en + test_en\n",
    "all_trg = train_vi + dev_vi + test_vi\n",
    "\n",
    "# Đường dẫn lưu file\n",
    "src_path = \"/kaggle/working/data/raw_data.src\"\n",
    "trg_path = \"/kaggle/working/data/raw_data.trg\"\n",
    "\n",
    "# Ghi file\n",
    "with open(src_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in all_src:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(trg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in all_trg:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(src_path)\n",
    "print(trg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f76d60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.126666Z",
     "iopub.status.busy": "2025-11-25T16:26:46.126075Z",
     "iopub.status.idle": "2025-11-25T16:26:46.130955Z",
     "shell.execute_reply": "2025-11-25T16:26:46.130167Z"
    },
    "papermill": {
     "duration": 0.010598,
     "end_time": "2025-11-25T16:26:46.132427",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.121829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And we did , we made a movie , which had to be the worst movie ever made in the history of movie making , but it was a blast .\n",
      "Here are some things you can say : \" I ca n't stop thinking about what happened last night .\n",
      "it 's about what we do right here , right now , and for the rest of our working lives .\n",
      "But if you stroll down to the Yamuna or to the Gomati in Lucknow , or to the Adyar river in Chennai , or the Mula - Mutha river in Pune , just see what we 're capable of doing to a river .\n",
      "Keep learning .\n",
      "HG : Indeed !\n",
      "And it 's becoming more and more difficult to understand the people who are not like you .\n",
      "The lighter the stain , the more oil content the stain has .\n",
      "If you do end up in the same place as your ex , notice his body language around you .\n",
      "Designers aspire to be really great designers .\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/working/data/raw_data.src\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f71728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.140826Z",
     "iopub.status.busy": "2025-11-25T16:26:46.140444Z",
     "iopub.status.idle": "2025-11-25T16:26:46.145045Z",
     "shell.execute_reply": "2025-11-25T16:26:46.144218Z"
    },
    "papermill": {
     "duration": 0.009957,
     "end_time": "2025-11-25T16:26:46.146244",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.136287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thế là chúng tôi đi , chúng tôi đã làm được , làm một bộ phim , chắc hẳn là bộ phim tồi nhất từng được làm trong lịch sử ngành điện ảnh .\n",
      "Đây là những điều bạn có thể nói : ' Anh không thể thôi nghĩ về những gì đã xảy ra tối qua .\n",
      "đó là về những gì chúng tôi phải ở đây , ngay bây giờ và cho phần còn lại của chúng tôi làm việc cuộc sống .\n",
      "Nhưng nếu bạn xuống sông Yamuna hoặc sông Gomati ở Lucknow hoặc sông Adyar ở Chennai hoặc sông Mula - Mutha ở Pune , bạn hãy xem những gì chúng ta có thể gây ra cho một dòng sông .\n",
      "Không ngừng học hỏi .\n",
      "HG : Quá chuẩn xác !\n",
      "Và càng lúc càng khó hơn để hiểu những người không thích bạn .\n",
      "Sơn càng nhạt màu thì càng chứa nhiều dầu .\n",
      "Nếu bạn có mặt tại cùng một nơi với người yêu cũ , bạn nên chú ý đến ngôn ngữ cơ thể của họ khi ở cạnh bạn .\n",
      "Những nhà thiết kế khao khát trở thành những nhà thiết kế vĩ đại .\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/working/data/raw_data.trg\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(10):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6523c2df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.153968Z",
     "iopub.status.busy": "2025-11-25T16:26:46.153791Z",
     "iopub.status.idle": "2025-11-25T16:26:46.157585Z",
     "shell.execute_reply": "2025-11-25T16:26:46.156871Z"
    },
    "papermill": {
     "duration": 0.008936,
     "end_time": "2025-11-25T16:26:46.158725",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.149789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số câu nguồn  (src): 700000\n",
      "Số câu đích   (trg): 700000\n"
     ]
    }
   ],
   "source": [
    "print(\"Số câu nguồn  (src):\", len(all_src))\n",
    "print(\"Số câu đích   (trg):\", len(all_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b4b6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.166824Z",
     "iopub.status.busy": "2025-11-25T16:26:46.166421Z",
     "iopub.status.idle": "2025-11-25T16:26:46.171646Z",
     "shell.execute_reply": "2025-11-25T16:26:46.170959Z"
    },
    "papermill": {
     "duration": 0.010479,
     "end_time": "2025-11-25T16:26:46.172734",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.162255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "import torch\n",
    "\n",
    "# Path or parameters for data\n",
    "DATA_DIR = 'data'\n",
    "SP_DIR = f'{DATA_DIR}/sp'\n",
    "SRC_DIR = 'src'\n",
    "TRG_DIR = 'trg'\n",
    "SRC_RAW_DATA_NAME = 'raw_data.src'\n",
    "TRG_RAW_DATA_NAME = 'raw_data.trg'\n",
    "TRAIN_NAME = 'train.txt'\n",
    "VALID_NAME = 'valid.txt'\n",
    "TEST_NAME = 'test.txt'\n",
    "\n",
    "# Parameters for sentencepiece tokenizer\n",
    "pad_id = 0\n",
    "sos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "src_model_prefix = 'src_sp'\n",
    "trg_model_prefix = 'trg_sp'\n",
    "sp_vocab_size = 16000\n",
    "character_coverage = 1.0\n",
    "model_type = 'unigram'\n",
    "\n",
    "# Parameters for Transformer & training\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "seq_len = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "d_k = d_model // num_heads\n",
    "drop_out_rate = 0.1\n",
    "num_epochs = 2\n",
    "beam_size = 5\n",
    "ckpt_dir = 'saved_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a708a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.180839Z",
     "iopub.status.busy": "2025-11-25T16:26:46.180428Z",
     "iopub.status.idle": "2025-11-25T16:26:46.185997Z",
     "shell.execute_reply": "2025-11-25T16:26:46.185328Z"
    },
    "papermill": {
     "duration": 0.010802,
     "end_time": "2025-11-25T16:26:46.187071",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.176269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_data.py\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from constants import SP_DIR, DATA_DIR, SRC_DIR, TRG_DIR, src_model_prefix, trg_model_prefix, batch_size, seq_len, pad_id, sos_id, eos_id\n",
    "\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "\n",
    "src_sp = spm.SentencePieceProcessor()\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "src_sp.Load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "trg_sp.Load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "\n",
    "\n",
    "def get_data_loader(file_name):\n",
    "    print(f\"Getting source/target {file_name}...\")\n",
    "    with open(f\"{DATA_DIR}/{SRC_DIR}/{file_name}\", 'r') as f:\n",
    "        src_text_list = f.readlines()\n",
    "\n",
    "    with open(f\"{DATA_DIR}/{TRG_DIR}/{file_name}\", 'r') as f:\n",
    "        trg_text_list = f.readlines()\n",
    "\n",
    "    print(\"Tokenizing & Padding src data...\")\n",
    "    src_list = process_src(src_text_list) # (sample_num, L)\n",
    "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
    "\n",
    "    print(\"Tokenizing & Padding trg data...\")\n",
    "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
    "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
    "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
    "\n",
    "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def pad_or_truncate(tokenized_text):\n",
    "    if len(tokenized_text) < seq_len:\n",
    "        left = seq_len - len(tokenized_text)\n",
    "        padding = [pad_id] * left\n",
    "        tokenized_text += padding\n",
    "    else:\n",
    "        tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def process_src(text_list):\n",
    "    tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = src_sp.EncodeAsIds(text.strip())\n",
    "        tokenized_list.append(pad_or_truncate(tokenized + [eos_id]))\n",
    "\n",
    "    return tokenized_list\n",
    "\n",
    "def process_trg(text_list):\n",
    "    input_tokenized_list = []\n",
    "    output_tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = trg_sp.EncodeAsIds(text.strip())\n",
    "        trg_input = [sos_id] + tokenized\n",
    "        trg_output = tokenized + [eos_id]\n",
    "        input_tokenized_list.append(pad_or_truncate(trg_input))\n",
    "        output_tokenized_list.append(pad_or_truncate(trg_output))\n",
    "\n",
    "    return input_tokenized_list, output_tokenized_list\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
    "        super().__init__()\n",
    "        self.src_data = torch.LongTensor(src_list)\n",
    "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
    "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
    "\n",
    "        assert np.shape(src_list) == np.shape(input_trg_list), \"The shape of src_list and input_trg_list are different.\"\n",
    "        assert np.shape(input_trg_list) == np.shape(output_trg_list), \"The shape of input_trg_list and output_trg_list are different.\"\n",
    "\n",
    "    def make_mask(self):\n",
    "        e_mask = (self.src_data != pad_id).unsqueeze(1) # (num_samples, 1, L)\n",
    "        d_mask = (self.input_trg_data != pad_id).unsqueeze(1) # (num_samples, 1, L)\n",
    "\n",
    "        nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool) # (1, L, L)\n",
    "        nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "        d_mask = d_mask & nopeak_mask # (num_samples, L, L) padding false\n",
    "\n",
    "        return e_mask, d_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.src_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93fb5d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.195596Z",
     "iopub.status.busy": "2025-11-25T16:26:46.195142Z",
     "iopub.status.idle": "2025-11-25T16:26:46.199725Z",
     "shell.execute_reply": "2025-11-25T16:26:46.199046Z"
    },
    "papermill": {
     "duration": 0.009931,
     "end_time": "2025-11-25T16:26:46.200814",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.190883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_structure.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_structure.py\n",
    "import heapq\n",
    "\n",
    "\n",
    "class BeamNode():\n",
    "    def __init__(self, cur_idx, prob, decoded):\n",
    "        self.cur_idx = cur_idx\n",
    "        self.prob = prob\n",
    "        self.decoded = decoded\n",
    "        self.is_finished = False\n",
    "        \n",
    "    def __gt__(self, other):\n",
    "        return self.prob > other.prob\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.prob >= other.prob\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.prob < other.prob\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        return self.prob <= other.prob\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.prob == other.prob\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return self.prob != other.prob\n",
    "    \n",
    "    def print_spec(self):\n",
    "        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n",
    "    \n",
    "\n",
    "class PriorityQueue():\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "        \n",
    "    def put(self, obj):\n",
    "        heapq.heappush(self.queue, (obj.prob, obj))\n",
    "        \n",
    "    def get(self):\n",
    "        return heapq.heappop(self.queue)[1]\n",
    "    \n",
    "    def qsize(self):\n",
    "        return len(self.queue)\n",
    "    \n",
    "    def print_scores(self):\n",
    "        scores = [t[0] for t in self.queue]\n",
    "        print(scores)\n",
    "        \n",
    "    def print_objs(self):\n",
    "        objs = [t[1] for t in self.queue]\n",
    "        print(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056313e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.209071Z",
     "iopub.status.busy": "2025-11-25T16:26:46.208857Z",
     "iopub.status.idle": "2025-11-25T16:26:46.214734Z",
     "shell.execute_reply": "2025-11-25T16:26:46.214060Z"
    },
    "papermill": {
     "duration": 0.011276,
     "end_time": "2025-11-25T16:26:46.215800",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.204524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile layers.py\n",
    "from torch import nn\n",
    "from constants import d_model, drop_out_rate, num_heads, d_k, d_ff, seq_len, device\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.multihead_attention = MultiheadAttention()\n",
    "        self.drop_out_1 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.feed_forward = FeedFowardLayer()\n",
    "        self.drop_out_2 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.masked_multihead_attention = MultiheadAttention()\n",
    "        self.drop_out_1 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.multihead_attention = MultiheadAttention()\n",
    "        self.drop_out_2 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_3 = LayerNormalization()\n",
    "        self.feed_forward = FeedFowardLayer()\n",
    "        self.drop_out_3 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x, e_output, e_mask,  d_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(\n",
    "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "\n",
    "        # W^Q, W^K, W^V in the paper\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Final output linear transformation\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        input_shape = q.shape\n",
    "\n",
    "        # Linear calculation +  split into num_heads\n",
    "        q = self.w_q(q).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "        k = self.w_k(k).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "        v = self.w_v(v).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "\n",
    "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Conduct self-attention\n",
    "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
    "        concat_output = attn_values.transpose(1, 2)\\\n",
    "            .contiguous().view(input_shape[0], -1, d_model) # (B, L, d_model)\n",
    "\n",
    "        return self.w_0(concat_output)\n",
    "\n",
    "    def self_attention(self, q, k, v, mask=None):\n",
    "        # Calculate attention scores with scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
    "        attn_scores = attn_scores / math.sqrt(d_k)\n",
    "\n",
    "        # If there is a mask, make masked spots -INF\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
    "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
    "\n",
    "        # Softmax and multiplying K to calculate attention value\n",
    "        attn_distribs = self.attn_softmax(attn_scores)\n",
    "\n",
    "        attn_distribs = self.dropout(attn_distribs)\n",
    "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
    "\n",
    "        return attn_values\n",
    "\n",
    "\n",
    "class FeedFowardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x) # (B, L, d_model)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.layer = nn.LayerNorm([d_model], elementwise_affine=True, eps=self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Make initial positional encoding matrix with 0\n",
    "        pe_matrix= torch.zeros(seq_len, d_model) # (L, d_model)\n",
    "\n",
    "        # Calculating position encoding values\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                elif i % 2 == 1:\n",
    "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / d_model)))\n",
    "\n",
    "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
    "        self.positional_encoding = pe_matrix.to(device=device).requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(d_model) # (B, L, d_model)\n",
    "        x = x + self.positional_encoding # (B, L, d_model)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3bbf42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.224716Z",
     "iopub.status.busy": "2025-11-25T16:26:46.224193Z",
     "iopub.status.idle": "2025-11-25T16:26:46.234337Z",
     "shell.execute_reply": "2025-11-25T16:26:46.233819Z"
    },
    "papermill": {
     "duration": 0.015719,
     "end_time": "2025-11-25T16:26:46.235297",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.219578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from tqdm import tqdm\n",
    "from constants import (\n",
    "    SP_DIR,\n",
    "    src_model_prefix,\n",
    "    trg_model_prefix,\n",
    "    seq_len,\n",
    "    pad_id,\n",
    "    sos_id,\n",
    "    eos_id,\n",
    "    learning_rate,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    ckpt_dir,\n",
    "    beam_size,\n",
    ")\n",
    "from constants import TRAIN_NAME, VALID_NAME\n",
    "from custom_data import get_data_loader, pad_or_truncate\n",
    "from transformer import Transformer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class Manager:\n",
    "    def __init__(self, is_train=True, ckpt_name=None):\n",
    "        # Load vocabs\n",
    "        print(\"Loading vocabs...\")\n",
    "        self.src_i2w = {}\n",
    "        self.trg_i2w = {}\n",
    "\n",
    "        with open(f\"{SP_DIR}/{src_model_prefix}.vocab\") as f:\n",
    "            lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            word = line.strip().split(\"\\t\")[0]\n",
    "            self.src_i2w[i] = word\n",
    "\n",
    "        with open(f\"{SP_DIR}/{trg_model_prefix}.vocab\") as f:\n",
    "            lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            word = line.strip().split(\"\\t\")[0]\n",
    "            self.trg_i2w[i] = word\n",
    "\n",
    "        print(\n",
    "            f\"The size of src vocab is {len(self.src_i2w)} and that of trg vocab is {len(self.trg_i2w)}.\"\n",
    "        )\n",
    "\n",
    "        # Load Transformer model & Adam optimizer\n",
    "        print(\"Loading Transformer model & Adam optimizer...\")\n",
    "        self.model = Transformer(\n",
    "            src_vocab_size=len(self.src_i2w), trg_vocab_size=len(self.trg_i2w)\n",
    "        ).to(device)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.best_loss = sys.float_info.max\n",
    "\n",
    "        if ckpt_name is not None:\n",
    "            assert os.path.exists(f\"{ckpt_dir}/{ckpt_name}\"), (\n",
    "                f\"There is no checkpoint named {ckpt_name}.\"\n",
    "            )\n",
    "\n",
    "            print(\"Loading checkpoint...\")\n",
    "            checkpoint = torch.load(\n",
    "                f\"{ckpt_dir}/{ckpt_name}\", map_location=device, weights_only=False\n",
    "            )\n",
    "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "            self.best_loss = checkpoint[\"loss\"]\n",
    "        else:\n",
    "            print(\"Initializing the model...\")\n",
    "            for p in self.model.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "        if is_train:\n",
    "            # Load loss function\n",
    "            print(\"Loading loss function...\")\n",
    "            self.criterion = nn.NLLLoss(ignore_index=pad_id)\n",
    "\n",
    "            # Load dataloaders\n",
    "            print(\"Loading dataloaders...\")\n",
    "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
    "            self.valid_loader = get_data_loader(VALID_NAME)\n",
    "\n",
    "        print(\"Setting finished.\")\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training starts.\")\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            self.model.train()\n",
    "\n",
    "            train_losses = []\n",
    "            start_time = datetime.datetime.now()\n",
    "\n",
    "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
    "                src_input, trg_input, trg_output = batch\n",
    "                src_input, trg_input, trg_output = (\n",
    "                    src_input.to(device),\n",
    "                    trg_input.to(device),\n",
    "                    trg_output.to(device),\n",
    "                )\n",
    "\n",
    "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
    "\n",
    "                output = self.model(\n",
    "                    src_input, trg_input, e_mask, d_mask\n",
    "                )  # (B, L, vocab_size)\n",
    "\n",
    "                trg_output_shape = trg_output.shape\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.criterion(\n",
    "                    output.view(-1, self.model.trg_vocab_size),\n",
    "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1]),\n",
    "                )\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time = end_time - start_time\n",
    "            seconds = training_time.seconds\n",
    "            hours = seconds // 3600\n",
    "            minutes = (seconds % 3600) // 60\n",
    "            seconds = seconds % 60\n",
    "\n",
    "            mean_train_loss = np.mean(train_losses)\n",
    "            print(f\"#################### Epoch: {epoch} ####################\")\n",
    "            print(\n",
    "                f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\"\n",
    "            )\n",
    "\n",
    "            valid_loss, valid_time = self.validation()\n",
    "\n",
    "            if valid_loss < self.best_loss:\n",
    "                if not os.path.exists(ckpt_dir):\n",
    "                    os.mkdir(ckpt_dir)\n",
    "\n",
    "                self.best_loss = valid_loss\n",
    "                state_dict = {\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optim_state_dict\": self.optim.state_dict(),\n",
    "                    \"loss\": self.best_loss,\n",
    "                }\n",
    "                torch.save(state_dict, f\"{ckpt_dir}/best_ckpt.tar\")\n",
    "                print(\"***** Current best checkpoint is saved. *****\")\n",
    "\n",
    "            print(f\"Best valid loss: {self.best_loss}\")\n",
    "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
    "\n",
    "        print(\"Training finished!\")\n",
    "\n",
    "    def validation(self):\n",
    "        print(\"Validation processing...\")\n",
    "        self.model.eval()\n",
    "\n",
    "        valid_losses = []\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
    "                src_input, trg_input, trg_output = batch\n",
    "                src_input, trg_input, trg_output = (\n",
    "                    src_input.to(device),\n",
    "                    trg_input.to(device),\n",
    "                    trg_output.to(device),\n",
    "                )\n",
    "\n",
    "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
    "\n",
    "                output = self.model(\n",
    "                    src_input, trg_input, e_mask, d_mask\n",
    "                )  # (B, L, vocab_size)\n",
    "\n",
    "                trg_output_shape = trg_output.shape\n",
    "                loss = self.criterion(\n",
    "                    output.view(-1, self.model.trg_vocab_size),\n",
    "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1]),\n",
    "                )\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        validation_time = end_time - start_time\n",
    "        seconds = validation_time.seconds\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        seconds = seconds % 60\n",
    "\n",
    "        mean_valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
    "\n",
    "    def inference(self, input_sentence, method):\n",
    "        print(\"Inference starts.\")\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loading sentencepiece tokenizer...\")\n",
    "        src_sp = spm.SentencePieceProcessor()\n",
    "        trg_sp = spm.SentencePieceProcessor()\n",
    "        src_sp.Load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "        trg_sp.Load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "\n",
    "        print(\"Preprocessing input sentence...\")\n",
    "        tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "        src_data = (\n",
    "            torch.LongTensor(pad_or_truncate(tokenized)).unsqueeze(0).to(device)\n",
    "        )  # (1, L)\n",
    "        e_mask = (src_data != pad_id).unsqueeze(1).to(device)  # (1, 1, L)\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        print(\"Encoding input sentence...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src_data = self.model.src_embedding(src_data)\n",
    "            src_data = self.model.positional_encoder(src_data)\n",
    "            e_output = self.model.encoder(src_data, e_mask)  # (1, L, d_model)\n",
    "\n",
    "            if method == \"greedy\":\n",
    "                print(\"Greedy decoding selected.\")\n",
    "                result = self.greedy_search(e_output, e_mask, trg_sp)\n",
    "            elif method == \"beam\":\n",
    "                print(\"Beam search selected.\")\n",
    "                result = self.beam_search(e_output, e_mask, trg_sp)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        total_inference_time = end_time - start_time\n",
    "        seconds = total_inference_time.seconds\n",
    "        minutes = seconds // 60\n",
    "        seconds = seconds % 60\n",
    "\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        print(\n",
    "            f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def greedy_search(self, e_output, e_mask, trg_sp):\n",
    "        last_words = torch.LongTensor([pad_id] * seq_len).to(device)  # (L)\n",
    "        last_words[0] = sos_id  # (L)\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            d_mask = (\n",
    "                (last_words.unsqueeze(0) != pad_id).unsqueeze(1).to(device)\n",
    "            )  # (1, 1, L)\n",
    "            nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool).to(\n",
    "                device\n",
    "            )  # (1, L, L)\n",
    "            nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "            trg_embedded = self.model.trg_embedding(last_words.unsqueeze(0))\n",
    "            trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n",
    "            decoder_output = self.model.decoder(\n",
    "                trg_positional_encoded, e_output, e_mask, d_mask\n",
    "            )  # (1, L, d_model)\n",
    "\n",
    "            output = self.model.softmax(\n",
    "                self.model.output_linear(decoder_output)\n",
    "            )  # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = torch.argmax(output, dim=-1)  # (1, L)\n",
    "            last_word_id = output[0][i].item()\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                last_words[i + 1] = last_word_id\n",
    "                cur_len += 1\n",
    "\n",
    "            if last_word_id == eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1].item() == pad_id:\n",
    "            decoded_output = last_words[1:cur_len].tolist()\n",
    "        else:\n",
    "            decoded_output = last_words[1:].tolist()\n",
    "        decoded_output = trg_sp.decode_ids(decoded_output)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "    def beam_search(self, e_output, e_mask, trg_sp, beam_size=beam_size, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Beam search implemented correctly.\n",
    "        - e_output: encoder outputs (1, L_enc, d_model)\n",
    "        - e_mask: encoder mask (1, 1, L_enc)\n",
    "        - trg_sp: SentencePiece processor for decoding ids->text\n",
    "        - beam_size: beam width\n",
    "        - alpha: length normalization hyperparameter (common default ~0.7)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Each hypothesis: (tokens_list, cumulative_logprob, is_finished)\n",
    "        # Initialize with single hypothesis [SOS]\n",
    "        hypotheses = [([sos_id], 0.0, False)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            all_candidates = []\n",
    "\n",
    "            # If all hypotheses are finished, we can stop early\n",
    "            if all(h[2] for h in hypotheses):\n",
    "                break\n",
    "\n",
    "            # Build batch of decoder inputs: for every hypothesis that is not finished,\n",
    "            # we'll expand with top-k next token candidates. For finished hypos, keep them as-is.\n",
    "            # We will run decoder on the batch of candidate sequences to get log-probs.\n",
    "            for h_idx, (tokens, logp, finished) in enumerate(hypotheses):\n",
    "                if finished:\n",
    "                    # keep finished hypothesis as a candidate (carry over)\n",
    "                    all_candidates.append((tokens, logp, True))\n",
    "                else:\n",
    "                    # We will expand this hypothesis; but first create its current input (padded)\n",
    "                    # We'll ask the model for top-k next tokens; to do that efficiently we will\n",
    "                    # create candidate inputs later.\n",
    "                    # For now just note we will expand this hypothesis.\n",
    "                    # We'll create the actual candidate inputs after we determine top-k per hypo.\n",
    "                    pass\n",
    "\n",
    "            # To get top-k for each hypothesis we need model output at position t given its tokens.\n",
    "            # We'll create a batch of current hypotheses (one per non-finished hypo), run decoder,\n",
    "            # and extract log-probs at time step t, then pick top-k per hypothesis.\n",
    "            alive_hypos = [(idx, h) for idx, h in enumerate(hypotheses) if not h[2]]\n",
    "            if len(alive_hypos) == 0:\n",
    "                break\n",
    "\n",
    "            # Prepare batch input: for each alive hypo, create padded tensor (seq_len) with its tokens\n",
    "            batch_inputs = []\n",
    "            hypo_map = []  # map from batch row -> hypothesis index\n",
    "            for h_idx, (tokens, logp, finished) in alive_hypos:\n",
    "                seq = tokens + [pad_id] * (seq_len - len(tokens))\n",
    "                batch_inputs.append(seq)\n",
    "                hypo_map.append(h_idx)\n",
    "\n",
    "            batch_inputs = torch.LongTensor(batch_inputs).to(\n",
    "                device\n",
    "            )  # (B_alive, seq_len)\n",
    "\n",
    "            # Create decoder mask for this batch\n",
    "            d_mask = (\n",
    "                (batch_inputs != pad_id).unsqueeze(1).to(device)\n",
    "            )  # (B_alive, 1, seq_len)\n",
    "            nopeak = torch.tril(\n",
    "                torch.ones((1, seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "            )\n",
    "            d_mask = d_mask & nopeak  # (B_alive, seq_len, seq_len) broadcasted\n",
    "\n",
    "            # Run decoder for this batch (one forward)\n",
    "            with torch.no_grad():\n",
    "                trg_emb = self.model.trg_embedding(\n",
    "                    batch_inputs\n",
    "                )  # (B_alive, L, d_model)\n",
    "                trg_emb = self.model.positional_encoder(\n",
    "                    trg_emb\n",
    "                )  # (B_alive, L, d_model)\n",
    "                dec_out = self.model.decoder(\n",
    "                    trg_emb,\n",
    "                    e_output.repeat(len(batch_inputs), 1, 1)\n",
    "                    if e_output.size(0) == 1\n",
    "                    else e_output,\n",
    "                    e_mask.repeat(len(batch_inputs), 1, 1)\n",
    "                    if e_mask.size(0) == 1\n",
    "                    else e_mask,\n",
    "                    d_mask,\n",
    "                )  # (B_alive, L, d_model)\n",
    "                # Get log-probs (use model's output_linear then log_softmax to be safe)\n",
    "                logits = self.model.output_linear(dec_out)  # (B_alive, L, V)\n",
    "                log_probs = F.log_softmax(logits, dim=-1)  # (B_alive, L, V)\n",
    "\n",
    "            # For each alive hypothesis, get top-k tokens at time step t\n",
    "            B_alive = log_probs.size(0)\n",
    "            V = log_probs.size(-1)\n",
    "            topk = min(beam_size, V)\n",
    "\n",
    "            for i in range(B_alive):\n",
    "                hypo_idx = hypo_map[i]\n",
    "                tokens, curr_logp, _ = hypotheses[hypo_idx]\n",
    "                # get log-prob vector at time t\n",
    "                logp_t = log_probs[i, t]  # (V,)\n",
    "                top_vals, top_idx = torch.topk(logp_t, k=topk)  # both tensors\n",
    "                top_vals = top_vals.cpu().tolist()\n",
    "                top_idx = top_idx.cpu().tolist()\n",
    "                for k_idx, token_id in enumerate(top_idx):\n",
    "                    new_tokens = tokens + [token_id]\n",
    "                    new_logp = curr_logp + top_vals[k_idx]  # cumulative log-prob\n",
    "                    finished = token_id == eos_id\n",
    "                    all_candidates.append((new_tokens, new_logp, finished))\n",
    "\n",
    "            # Also include previous finished hypotheses (they were added earlier)\n",
    "\n",
    "            # Now select top `beam_size` candidates among all_candidates by cumulative log-prob\n",
    "            # Note: do NOT normalize length here (we keep cumulative log-prob for beam propagation).\n",
    "            all_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "            hypotheses = all_candidates[:beam_size]\n",
    "\n",
    "            # If number of hypotheses < beam_size (possible if many finished), pad by carrying best finished\n",
    "            # (not strictly necessary)\n",
    "\n",
    "        # After finishing (either reached seq_len or all finished), choose best hypothesis with length normalization\n",
    "        # Apply length normalization score = logp / (len(tokens) ** alpha)\n",
    "        final_scores = []\n",
    "        for tokens, logp, finished in hypotheses:\n",
    "            length = len(tokens) - 1  # exclude SOS for length\n",
    "            if length <= 0:\n",
    "                length = 1.0\n",
    "            score = logp / (length**alpha)\n",
    "            final_scores.append((score, tokens, finished))\n",
    "\n",
    "        final_scores = sorted(final_scores, key=lambda x: x[0], reverse=True)\n",
    "        best_tokens = final_scores[0][1]\n",
    "\n",
    "        # Remove leading SOS and trailing EOS if present\n",
    "        if best_tokens and best_tokens[0] == sos_id:\n",
    "            best_tokens = best_tokens[1:]\n",
    "        if best_tokens and best_tokens[-1] == eos_id:\n",
    "            best_tokens = best_tokens[:-1]\n",
    "\n",
    "        return trg_sp.decode_ids(best_tokens)\n",
    "\n",
    "    def make_mask(self, src_input, trg_input):\n",
    "        e_mask = (src_input != pad_id).unsqueeze(1).to(device)\n",
    "        d_mask = (trg_input != pad_id).unsqueeze(1).to(device)\n",
    "\n",
    "        nopeak_mask = torch.tril(\n",
    "            torch.ones((1, seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "        )  # (1, L, L) to triangular shape\n",
    "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
    "\n",
    "        return e_mask, d_mask\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", required=True, help=\"train or inference?\")\n",
    "    parser.add_argument(\"--ckpt_name\", required=False, help=\"best checkpoint file\")\n",
    "    parser.add_argument(\n",
    "        \"--input\", type=str, required=False, help=\"input sentence when inferencing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decode\", type=str, required=False, default=\"greedy\", help=\"greedy or beam?\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        if args.ckpt_name is not None:\n",
    "            manager = Manager(is_train=True, ckpt_name=args.ckpt_name)\n",
    "        else:\n",
    "            manager = Manager(is_train=True)\n",
    "\n",
    "        manager.train()\n",
    "    elif args.mode == \"inference\":\n",
    "        assert args.ckpt_name is not None, (\n",
    "            \"Please specify the model file name you want to use.\"\n",
    "        )\n",
    "        assert args.input is not None, \"Please specify the input sentence to translate.\"\n",
    "        assert args.decode == \"greedy\" or args.decode == \"beam\", (\n",
    "            \"Please specify correct decoding method, either 'greedy' or 'beam'.\"\n",
    "        )\n",
    "\n",
    "        manager = Manager(is_train=False, ckpt_name=args.ckpt_name)\n",
    "        manager.inference(args.input, args.decode)\n",
    "\n",
    "    else:\n",
    "        print(\"Please specify mode argument either with 'train' or 'inference'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13535932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.243887Z",
     "iopub.status.busy": "2025-11-25T16:26:46.243698Z",
     "iopub.status.idle": "2025-11-25T16:26:46.248122Z",
     "shell.execute_reply": "2025-11-25T16:26:46.247589Z"
    },
    "papermill": {
     "duration": 0.009948,
     "end_time": "2025-11-25T16:26:46.249055",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.239107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sentencepiece_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentencepiece_train.py\n",
    "from constants import DATA_DIR, SP_DIR, SRC_RAW_DATA_NAME, TRG_RAW_DATA_NAME\n",
    "from constants import src_model_prefix, trg_model_prefix, pad_id, sos_id, eos_id, unk_id, sp_vocab_size, character_coverage, model_type\n",
    "from constants import SRC_DIR, TRG_DIR, TRAIN_NAME, VALID_NAME\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "train_frac = 0.8\n",
    "\n",
    "def train_sp(is_src=True):\n",
    "    template = \"--input={} \\\n",
    "                --pad_id={} \\\n",
    "                --bos_id={} \\\n",
    "                --eos_id={} \\\n",
    "                --unk_id={} \\\n",
    "                --model_prefix={} \\\n",
    "                --vocab_size={} \\\n",
    "                --character_coverage={} \\\n",
    "                --model_type={}\"\n",
    "\n",
    "    if is_src:\n",
    "        this_input_file = f\"{DATA_DIR}/{SRC_RAW_DATA_NAME}\"\n",
    "        this_model_prefix = f\"{SP_DIR}/{src_model_prefix}\"\n",
    "    else:\n",
    "        this_input_file = f\"{DATA_DIR}/{TRG_RAW_DATA_NAME}\"\n",
    "        this_model_prefix = f\"{SP_DIR}/{trg_model_prefix}\"\n",
    "\n",
    "    config = template.format(this_input_file,\n",
    "                            pad_id,\n",
    "                            sos_id,\n",
    "                            eos_id,\n",
    "                            unk_id,\n",
    "                            this_model_prefix,\n",
    "                            sp_vocab_size,\n",
    "                            character_coverage,\n",
    "                            model_type)\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    if not os.path.isdir(SP_DIR):\n",
    "        os.mkdir(SP_DIR)\n",
    "\n",
    "    print(spm)\n",
    "    spm.SentencePieceTrainer.Train(config)\n",
    "    \n",
    "    \n",
    "def split_data(raw_data_name, data_dir):\n",
    "    with open(f\"{DATA_DIR}/{raw_data_name}\") as f:\n",
    "        lines = f.readlines()    \n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    \n",
    "    train_lines = lines[:int(train_frac * len(lines))]\n",
    "    valid_lines = lines[int(train_frac * len(lines)):]\n",
    "    \n",
    "    if not os.path.isdir(f\"{DATA_DIR}/{data_dir}\"):\n",
    "        os.mkdir(f\"{DATA_DIR}/{data_dir}\")\n",
    "    \n",
    "    with open(f\"{DATA_DIR}/{data_dir}/{TRAIN_NAME}\", 'w') as f:\n",
    "        for line in tqdm(train_lines):\n",
    "            f.write(line.strip() + '\\n')\n",
    "            \n",
    "    with open(f\"{DATA_DIR}/{data_dir}/{VALID_NAME}\", 'w') as f:\n",
    "        for line in tqdm(valid_lines):\n",
    "            f.write(line.strip() + '\\n')\n",
    "            \n",
    "    print(f\"Train/Validation data saved in {DATA_DIR}/{data_dir}.\")\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_sp(is_src=True)\n",
    "    train_sp(is_src=False)\n",
    "    split_data(SRC_RAW_DATA_NAME, SRC_DIR)\n",
    "    split_data(TRG_RAW_DATA_NAME, TRG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d801ec34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.257893Z",
     "iopub.status.busy": "2025-11-25T16:26:46.257483Z",
     "iopub.status.idle": "2025-11-25T16:26:46.262055Z",
     "shell.execute_reply": "2025-11-25T16:26:46.261356Z"
    },
    "papermill": {
     "duration": 0.010143,
     "end_time": "2025-11-25T16:26:46.263148",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.253005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformer.py\n",
    "from torch import nn\n",
    "from constants import d_model, num_layers\n",
    "from layers import EncoderLayer, DecoderLayer, PositionalEncoder, LayerNormalization\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "        self.src_embedding = nn.Embedding(self.src_vocab_size, d_model)\n",
    "        self.trg_embedding = nn.Embedding(self.trg_vocab_size, d_model)\n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.output_linear = nn.Linear(d_model, self.trg_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
    "        src_input = self.src_embedding(src_input) # (B, L) => (B, L, d_model)\n",
    "        trg_input = self.trg_embedding(trg_input) # (B, L) => (B, L, d_model)\n",
    "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
    "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
    "\n",
    "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
    "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
    "\n",
    "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for i in range(num_layers)])\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        for i in range(num_layers):\n",
    "            x = self.layers[i](x, e_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for i in range(num_layers)])\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, e_output, e_mask, d_mask):\n",
    "        for i in range(num_layers):\n",
    "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
    "\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fea7664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.271804Z",
     "iopub.status.busy": "2025-11-25T16:26:46.271621Z",
     "iopub.status.idle": "2025-11-25T16:26:46.402674Z",
     "shell.execute_reply": "2025-11-25T16:26:46.401937Z"
    },
    "papermill": {
     "duration": 0.136868,
     "end_time": "2025-11-25T16:26:46.403910",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.267042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 148196\r\n",
      "-rw-r--r-- 1 root root 65231201 Nov 25 16:26 raw_data.src\r\n",
      "-rw-r--r-- 1 root root 86504878 Nov 25 16:26 raw_data.trg\r\n",
      "drwxr-xr-x 2 root root     4096 Nov 25 16:26 sp\r\n",
      "drwxr-xr-x 2 root root     4096 Nov 25 16:26 src\r\n",
      "drwxr-xr-x 2 root root     4096 Nov 25 16:26 trg\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200ec66b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T16:26:46.415604Z",
     "iopub.status.busy": "2025-11-25T16:26:46.415212Z",
     "iopub.status.idle": "2025-11-25T16:28:41.184419Z",
     "shell.execute_reply": "2025-11-25T16:28:41.183705Z"
    },
    "papermill": {
     "duration": 114.777498,
     "end_time": "2025-11-25T16:28:41.185872",
     "exception": false,
     "start_time": "2025-11-25T16:26:46.408374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input=data/raw_data.src                 --pad_id=0                 --bos_id=1                 --eos_id=2                 --unk_id=3                 --model_prefix=data/sp/src_sp                 --vocab_size=16000                 --character_coverage=1.0                 --model_type=unigram\r\n",
      "<module 'sentencepiece' from '/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py'>\r\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=data/raw_data.src                 --pad_id=0                 --bos_id=1                 --eos_id=2                 --unk_id=3                 --model_prefix=data/sp/src_sp                 --vocab_size=16000                 --character_coverage=1.0                 --model_type=unigram\r\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \r\n",
      "trainer_spec {\r\n",
      "  input: data/raw_data.src\r\n",
      "  input_format: \r\n",
      "  model_prefix: data/sp/src_sp\r\n",
      "  model_type: UNIGRAM\r\n",
      "  vocab_size: 16000\r\n",
      "  self_test_sample_size: 0\r\n",
      "  character_coverage: 1\r\n",
      "  input_sentence_size: 0\r\n",
      "  shuffle_input_sentence: 1\r\n",
      "  seed_sentencepiece_size: 1000000\r\n",
      "  shrinking_factor: 0.75\r\n",
      "  max_sentence_length: 4192\r\n",
      "  num_threads: 16\r\n",
      "  num_sub_iterations: 2\r\n",
      "  max_sentencepiece_length: 16\r\n",
      "  split_by_unicode_script: 1\r\n",
      "  split_by_number: 1\r\n",
      "  split_by_whitespace: 1\r\n",
      "  split_digits: 0\r\n",
      "  pretokenization_delimiter: \r\n",
      "  treat_whitespace_as_suffix: 0\r\n",
      "  allow_whitespace_only_pieces: 0\r\n",
      "  required_chars: \r\n",
      "  byte_fallback: 0\r\n",
      "  vocabulary_output_piece_score: 1\r\n",
      "  train_extremely_large_corpus: 0\r\n",
      "  seed_sentencepieces_file: \r\n",
      "  hard_vocab_limit: 1\r\n",
      "  use_all_vocab: 0\r\n",
      "  unk_id: 3\r\n",
      "  bos_id: 1\r\n",
      "  eos_id: 2\r\n",
      "  pad_id: 0\r\n",
      "  unk_piece: <unk>\r\n",
      "  bos_piece: <s>\r\n",
      "  eos_piece: </s>\r\n",
      "  pad_piece: <pad>\r\n",
      "  unk_surface:  ⁇ \r\n",
      "  enable_differential_privacy: 0\r\n",
      "  differential_privacy_noise_level: 0\r\n",
      "  differential_privacy_clipping_threshold: 0\r\n",
      "}\r\n",
      "normalizer_spec {\r\n",
      "  name: nmt_nfkc\r\n",
      "  add_dummy_prefix: 1\r\n",
      "  remove_extra_whitespaces: 1\r\n",
      "  escape_whitespaces: 1\r\n",
      "  normalization_rule_tsv: \r\n",
      "}\r\n",
      "denormalizer_spec {}\r\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\r\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/raw_data.src\r\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 700000 sentences\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\r\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\r\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=65201616\r\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.\r\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=440\r\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\r\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 700000 sentences.\r\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\r\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=32760865\r\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 166918 seed sentencepieces\r\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 700000\r\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 100588\r\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 100588 sentences for EM training\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=74746 obj=9.44002 num_tokens=197992 num_tokens/piece=2.64886\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=55150 obj=7.54703 num_tokens=198609 num_tokens/piece=3.60125\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=41355 obj=7.46966 num_tokens=207643 num_tokens/piece=5.02099\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=41325 obj=7.45796 num_tokens=207806 num_tokens/piece=5.02858\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=30992 obj=7.47468 num_tokens=228954 num_tokens/piece=7.38752\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=30992 obj=7.46957 num_tokens=228912 num_tokens/piece=7.38616\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=23244 obj=7.5042 num_tokens=253206 num_tokens/piece=10.8934\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=23244 obj=7.49673 num_tokens=253124 num_tokens/piece=10.8899\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17600 obj=7.55361 num_tokens=277027 num_tokens/piece=15.7402\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17600 obj=7.5423 num_tokens=276929 num_tokens/piece=15.7346\r\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: data/sp/src_sp.model\r\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: data/sp/src_sp.vocab\r\n",
      "--input=data/raw_data.trg                 --pad_id=0                 --bos_id=1                 --eos_id=2                 --unk_id=3                 --model_prefix=data/sp/trg_sp                 --vocab_size=16000                 --character_coverage=1.0                 --model_type=unigram\r\n",
      "<module 'sentencepiece' from '/usr/local/lib/python3.11/dist-packages/sentencepiece/__init__.py'>\r\n",
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=data/raw_data.trg                 --pad_id=0                 --bos_id=1                 --eos_id=2                 --unk_id=3                 --model_prefix=data/sp/trg_sp                 --vocab_size=16000                 --character_coverage=1.0                 --model_type=unigram\r\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \r\n",
      "trainer_spec {\r\n",
      "  input: data/raw_data.trg\r\n",
      "  input_format: \r\n",
      "  model_prefix: data/sp/trg_sp\r\n",
      "  model_type: UNIGRAM\r\n",
      "  vocab_size: 16000\r\n",
      "  self_test_sample_size: 0\r\n",
      "  character_coverage: 1\r\n",
      "  input_sentence_size: 0\r\n",
      "  shuffle_input_sentence: 1\r\n",
      "  seed_sentencepiece_size: 1000000\r\n",
      "  shrinking_factor: 0.75\r\n",
      "  max_sentence_length: 4192\r\n",
      "  num_threads: 16\r\n",
      "  num_sub_iterations: 2\r\n",
      "  max_sentencepiece_length: 16\r\n",
      "  split_by_unicode_script: 1\r\n",
      "  split_by_number: 1\r\n",
      "  split_by_whitespace: 1\r\n",
      "  split_digits: 0\r\n",
      "  pretokenization_delimiter: \r\n",
      "  treat_whitespace_as_suffix: 0\r\n",
      "  allow_whitespace_only_pieces: 0\r\n",
      "  required_chars: \r\n",
      "  byte_fallback: 0\r\n",
      "  vocabulary_output_piece_score: 1\r\n",
      "  train_extremely_large_corpus: 0\r\n",
      "  seed_sentencepieces_file: \r\n",
      "  hard_vocab_limit: 1\r\n",
      "  use_all_vocab: 0\r\n",
      "  unk_id: 3\r\n",
      "  bos_id: 1\r\n",
      "  eos_id: 2\r\n",
      "  pad_id: 0\r\n",
      "  unk_piece: <unk>\r\n",
      "  bos_piece: <s>\r\n",
      "  eos_piece: </s>\r\n",
      "  pad_piece: <pad>\r\n",
      "  unk_surface:  ⁇ \r\n",
      "  enable_differential_privacy: 0\r\n",
      "  differential_privacy_noise_level: 0\r\n",
      "  differential_privacy_clipping_threshold: 0\r\n",
      "}\r\n",
      "normalizer_spec {\r\n",
      "  name: nmt_nfkc\r\n",
      "  add_dummy_prefix: 1\r\n",
      "  remove_extra_whitespaces: 1\r\n",
      "  escape_whitespaces: 1\r\n",
      "  normalization_rule_tsv: \r\n",
      "}\r\n",
      "denormalizer_spec {}\r\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\r\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data/raw_data.trg\r\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 700000 sentences\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\r\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\r\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\r\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=65167431\r\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.\r\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=519\r\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\r\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 700000 sentences.\r\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\r\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=30244196\r\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 74717 seed sentencepieces\r\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 700000\r\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 55022\r\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 55022 sentences for EM training\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=40061 obj=8.55726 num_tokens=116215 num_tokens/piece=2.90095\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33077 obj=7.1987 num_tokens=115071 num_tokens/piece=3.47888\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=24794 obj=7.12254 num_tokens=119483 num_tokens/piece=4.81903\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=24741 obj=7.11346 num_tokens=119448 num_tokens/piece=4.82794\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=18554 obj=7.11872 num_tokens=129218 num_tokens/piece=6.96443\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=18554 obj=7.11597 num_tokens=129184 num_tokens/piece=6.9626\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17600 obj=7.1169 num_tokens=130919 num_tokens/piece=7.43858\r\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=17600 obj=7.11615 num_tokens=130925 num_tokens/piece=7.43892\r\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: data/sp/trg_sp.model\r\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: data/sp/trg_sp.vocab\r\n",
      "Splitting data...\r\n",
      "100%|██████████████████████████████| 560000/560000 [00:00<00:00, 2530243.91it/s]\r\n",
      "100%|██████████████████████████████| 140000/140000 [00:00<00:00, 2316042.87it/s]\r\n",
      "Train/Validation data saved in data/src.\r\n",
      "Splitting data...\r\n",
      "100%|██████████████████████████████| 560000/560000 [00:00<00:00, 1148205.37it/s]\r\n",
      "100%|██████████████████████████████| 140000/140000 [00:00<00:00, 1089730.85it/s]\r\n",
      "Train/Validation data saved in data/trg.\r\n"
     ]
    }
   ],
   "source": [
    "!python sentencepiece_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0dffe",
   "metadata": {
    "papermill": {
     "duration": 0.005919,
     "end_time": "2025-11-25T16:28:41.198339",
     "exception": false,
     "start_time": "2025-11-25T16:28:41.192420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e865c47",
   "metadata": {
    "papermill": {
     "duration": 0.005795,
     "end_time": "2025-11-25T16:28:41.210129",
     "exception": false,
     "start_time": "2025-11-25T16:28:41.204334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 280819909,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 125.498492,
   "end_time": "2025-11-25T16:28:41.734052",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T16:26:36.235560",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
