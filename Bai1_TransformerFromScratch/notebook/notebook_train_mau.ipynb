{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88d00df",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:36.663349Z",
     "iopub.status.busy": "2025-11-29T03:47:36.663094Z",
     "iopub.status.idle": "2025-11-29T03:47:43.087399Z",
     "shell.execute_reply": "2025-11-29T03:47:43.086830Z"
    },
    "papermill": {
     "duration": 6.430009,
     "end_time": "2025-11-29T03:47:43.088802",
     "exception": false,
     "start_time": "2025-11-29T03:47:36.658793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd595004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:43.095700Z",
     "iopub.status.busy": "2025-11-29T03:47:43.095407Z",
     "iopub.status.idle": "2025-11-29T03:47:45.624106Z",
     "shell.execute_reply": "2025-11-29T03:47:45.623225Z"
    },
    "papermill": {
     "duration": 2.533758,
     "end_time": "2025-11-29T03:47:45.625673",
     "exception": false,
     "start_time": "2025-11-29T03:47:43.091915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf data\n",
    "!mkdir data\n",
    "!cp -r /kaggle/input/create-phomt-700k-pairs/data/* ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c323c551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.632631Z",
     "iopub.status.busy": "2025-11-29T03:47:45.632343Z",
     "iopub.status.idle": "2025-11-29T03:47:45.759197Z",
     "shell.execute_reply": "2025-11-29T03:47:45.758493Z"
    },
    "papermill": {
     "duration": 0.131874,
     "end_time": "2025-11-29T03:47:45.760409",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.628535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\r\n",
      "raw_data.src  raw_data.trg  sp\tsrc  trg\r\n",
      "\r\n",
      "data/sp:\r\n",
      "src_sp.model  src_sp.vocab  trg_sp.model  trg_sp.vocab\r\n",
      "\r\n",
      "data/src:\r\n",
      "train.txt  valid.txt\r\n",
      "\r\n",
      "data/trg:\r\n",
      "train.txt  valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2f9102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.767241Z",
     "iopub.status.busy": "2025-11-29T03:47:45.766984Z",
     "iopub.status.idle": "2025-11-29T03:47:45.773163Z",
     "shell.execute_reply": "2025-11-29T03:47:45.772362Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.01099,
     "end_time": "2025-11-29T03:47:45.774228",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.763238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "import torch\n",
    "\n",
    "# Path or parameters for data\n",
    "DATA_DIR = 'data'\n",
    "SP_DIR = f'{DATA_DIR}/sp'\n",
    "SRC_DIR = 'src'\n",
    "TRG_DIR = 'trg'\n",
    "SRC_RAW_DATA_NAME = 'raw_data.src'\n",
    "TRG_RAW_DATA_NAME = 'raw_data.trg'\n",
    "TRAIN_NAME = 'train.txt'\n",
    "VALID_NAME = 'valid.txt'\n",
    "TEST_NAME = 'test.txt'\n",
    "\n",
    "# Parameters for sentencepiece tokenizer\n",
    "pad_id = 0\n",
    "sos_id = 1\n",
    "eos_id = 2\n",
    "unk_id = 3\n",
    "src_model_prefix = 'src_sp'\n",
    "trg_model_prefix = 'trg_sp'\n",
    "sp_vocab_size = 16000\n",
    "character_coverage = 1.0\n",
    "model_type = 'unigram'\n",
    "\n",
    "# Parameters for Transformer & training\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "seq_len = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "d_k = d_model // num_heads\n",
    "drop_out_rate = 0.1\n",
    "num_epochs = 5\n",
    "beam_size = 5\n",
    "ckpt_dir = 'saved_model'\n",
    "\n",
    "# Others\n",
    "attention_type = 'luong' # 'bahdanau' or 'luong' or 'scaled_dot_product'\n",
    "start_epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0517a5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.780997Z",
     "iopub.status.busy": "2025-11-29T03:47:45.780676Z",
     "iopub.status.idle": "2025-11-29T03:47:45.785637Z",
     "shell.execute_reply": "2025-11-29T03:47:45.784946Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.009604,
     "end_time": "2025-11-29T03:47:45.786711",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.777107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom_data.py\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from constants import SP_DIR, DATA_DIR, SRC_DIR, TRG_DIR, src_model_prefix, trg_model_prefix, batch_size, seq_len, pad_id, sos_id, eos_id\n",
    "\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "\n",
    "src_sp = spm.SentencePieceProcessor()\n",
    "trg_sp = spm.SentencePieceProcessor()\n",
    "src_sp.Load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "trg_sp.Load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "\n",
    "\n",
    "def get_data_loader(file_name):\n",
    "    print(f\"Getting source/target {file_name}...\")\n",
    "    with open(f\"{DATA_DIR}/{SRC_DIR}/{file_name}\", 'r') as f:\n",
    "        src_text_list = f.readlines()\n",
    "\n",
    "    with open(f\"{DATA_DIR}/{TRG_DIR}/{file_name}\", 'r') as f:\n",
    "        trg_text_list = f.readlines()\n",
    "\n",
    "    print(\"Tokenizing & Padding src data...\")\n",
    "    src_list = process_src(src_text_list) # (sample_num, L)\n",
    "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
    "\n",
    "    print(\"Tokenizing & Padding trg data...\")\n",
    "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
    "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
    "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
    "\n",
    "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def pad_or_truncate(tokenized_text):\n",
    "    if len(tokenized_text) < seq_len:\n",
    "        left = seq_len - len(tokenized_text)\n",
    "        padding = [pad_id] * left\n",
    "        tokenized_text += padding\n",
    "    else:\n",
    "        tokenized_text = tokenized_text[:seq_len]\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "def process_src(text_list):\n",
    "    tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = src_sp.EncodeAsIds(text.strip())\n",
    "        tokenized_list.append(pad_or_truncate(tokenized + [eos_id]))\n",
    "\n",
    "    return tokenized_list\n",
    "\n",
    "def process_trg(text_list):\n",
    "    input_tokenized_list = []\n",
    "    output_tokenized_list = []\n",
    "    for text in tqdm(text_list):\n",
    "        tokenized = trg_sp.EncodeAsIds(text.strip())\n",
    "        trg_input = [sos_id] + tokenized\n",
    "        trg_output = tokenized + [eos_id]\n",
    "        input_tokenized_list.append(pad_or_truncate(trg_input))\n",
    "        output_tokenized_list.append(pad_or_truncate(trg_output))\n",
    "\n",
    "    return input_tokenized_list, output_tokenized_list\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
    "        super().__init__()\n",
    "        self.src_data = torch.LongTensor(src_list)\n",
    "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
    "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
    "\n",
    "        assert np.shape(src_list) == np.shape(input_trg_list), \"The shape of src_list and input_trg_list are different.\"\n",
    "        assert np.shape(input_trg_list) == np.shape(output_trg_list), \"The shape of input_trg_list and output_trg_list are different.\"\n",
    "\n",
    "    def make_mask(self):\n",
    "        e_mask = (self.src_data != pad_id).unsqueeze(1) # (num_samples, 1, L)\n",
    "        d_mask = (self.input_trg_data != pad_id).unsqueeze(1) # (num_samples, 1, L)\n",
    "\n",
    "        nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool) # (1, L, L)\n",
    "        nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
    "        d_mask = d_mask & nopeak_mask # (num_samples, L, L) padding false\n",
    "\n",
    "        return e_mask, d_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.shape(self.src_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4328559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.793269Z",
     "iopub.status.busy": "2025-11-29T03:47:45.792849Z",
     "iopub.status.idle": "2025-11-29T03:47:45.797475Z",
     "shell.execute_reply": "2025-11-29T03:47:45.796848Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.009003,
     "end_time": "2025-11-29T03:47:45.798481",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.789478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_structure.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_structure.py\n",
    "import heapq\n",
    "\n",
    "\n",
    "class BeamNode():\n",
    "    def __init__(self, cur_idx, prob, decoded):\n",
    "        self.cur_idx = cur_idx\n",
    "        self.prob = prob\n",
    "        self.decoded = decoded\n",
    "        self.is_finished = False\n",
    "        \n",
    "    def __gt__(self, other):\n",
    "        return self.prob > other.prob\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.prob >= other.prob\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.prob < other.prob\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        return self.prob <= other.prob\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.prob == other.prob\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return self.prob != other.prob\n",
    "    \n",
    "    def print_spec(self):\n",
    "        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n",
    "    \n",
    "\n",
    "class PriorityQueue():\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "        \n",
    "    def put(self, obj):\n",
    "        heapq.heappush(self.queue, (obj.prob, obj))\n",
    "        \n",
    "    def get(self):\n",
    "        return heapq.heappop(self.queue)[1]\n",
    "    \n",
    "    def qsize(self):\n",
    "        return len(self.queue)\n",
    "    \n",
    "    def print_scores(self):\n",
    "        scores = [t[0] for t in self.queue]\n",
    "        print(scores)\n",
    "        \n",
    "    def print_objs(self):\n",
    "        objs = [t[1] for t in self.queue]\n",
    "        print(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5fdbaac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.805210Z",
     "iopub.status.busy": "2025-11-29T03:47:45.804940Z",
     "iopub.status.idle": "2025-11-29T03:47:45.813128Z",
     "shell.execute_reply": "2025-11-29T03:47:45.812571Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.012878,
     "end_time": "2025-11-29T03:47:45.814165",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.801287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing layers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile layers.py\n",
    "from torch import nn\n",
    "from constants import d_model, drop_out_rate, num_heads, d_k, d_ff, seq_len, device, attention_type\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        if attention_type == 'bahdanau':\n",
    "            self.multihead_attention = BahdanauMultiheadAttention()\n",
    "        elif attention_type == 'scaled_dot_product':\n",
    "            self.multihead_attention = MultiheadAttention()\n",
    "        elif attention_type == 'luong':\n",
    "            self.multihead_attention = LuongMultiheadAttention()\n",
    "        self.drop_out_1 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.feed_forward = FeedFowardLayer()\n",
    "        self.drop_out_2 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = LayerNormalization()\n",
    "        self.masked_multihead_attention = None\n",
    "        if attention_type == 'bahdanau':\n",
    "            self.masked_multihead_attention = BahdanauMultiheadAttention()\n",
    "        elif attention_type == 'scaled_dot_product':\n",
    "            self.masked_multihead_attention = MultiheadAttention()\n",
    "        elif attention_type == 'luong':\n",
    "            self.masked_multihead_attention = LuongMultiheadAttention()\n",
    "        self.drop_out_1 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_2 = LayerNormalization()\n",
    "        self.multihead_attention = None\n",
    "        if attention_type == 'bahdanau':\n",
    "            self.multihead_attention = BahdanauMultiheadAttention()\n",
    "        elif attention_type == 'scaled_dot_product':\n",
    "            self.multihead_attention = MultiheadAttention()\n",
    "        elif attention_type == 'luong':\n",
    "            self.multihead_attention = LuongMultiheadAttention()\n",
    "        self.drop_out_2 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "        self.layer_norm_3 = LayerNormalization()\n",
    "        self.feed_forward = FeedFowardLayer()\n",
    "        self.drop_out_3 = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x, e_output, e_mask,  d_mask):\n",
    "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_1(\n",
    "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_2(\n",
    "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
    "        ) # (B, L, d_model)\n",
    "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
    "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
    "\n",
    "        return x # (B, L, d_model)\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "\n",
    "        # W^Q, W^K, W^V in the paper\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Final output linear transformation\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        input_shape = q.shape\n",
    "\n",
    "        # Linear calculation +  split into num_heads\n",
    "        q = self.w_q(q).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "        k = self.w_k(k).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "        v = self.w_v(v).view(input_shape[0], -1, num_heads, d_k) # (B, L, num_heads, d_k)\n",
    "\n",
    "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Conduct self-attention\n",
    "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
    "        concat_output = attn_values.transpose(1, 2)\\\n",
    "            .contiguous().view(input_shape[0], -1, d_model) # (B, L, d_model)\n",
    "\n",
    "        return self.w_0(concat_output)\n",
    "\n",
    "    def self_attention(self, q, k, v, mask=None):\n",
    "        # Calculate attention scores with scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
    "        attn_scores = attn_scores / math.sqrt(d_k)\n",
    "\n",
    "        # If there is a mask, make masked spots -INF\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
    "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
    "\n",
    "        # Softmax and multiplying K to calculate attention value\n",
    "        attn_distribs = self.attn_softmax(attn_scores)\n",
    "\n",
    "        attn_distribs = self.dropout(attn_distribs)\n",
    "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
    "\n",
    "        return attn_values\n",
    "\n",
    "\n",
    "class FeedFowardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x) # (B, L, d_model)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.layer = nn.LayerNorm([d_model], elementwise_affine=True, eps=self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Make initial positional encoding matrix with 0\n",
    "        pe_matrix= torch.zeros(seq_len, d_model) # (L, d_model)\n",
    "\n",
    "        # Calculating position encoding values\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(d_model):\n",
    "                if i % 2 == 0:\n",
    "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
    "                elif i % 2 == 1:\n",
    "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / d_model)))\n",
    "\n",
    "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
    "        # self.positional_encoding = pe_matrix.to(device=device).requires_grad_(False)\n",
    "        self.register_buffer('positional_encoding', pe_matrix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(d_model) # (B, L, d_model)\n",
    "        x = x + self.positional_encoding # (B, L, d_model)\n",
    "\n",
    "        return x\n",
    "\n",
    "class BahdanauMultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Bahdanau specific: Vector v (learned parameter)\n",
    "        # Shape (1, num_heads, 1, 1, d_k) for broadcasting\n",
    "        self.v = nn.Parameter(torch.rand(1, num_heads, 1, 1, d_k))\n",
    "        nn.init.xavier_uniform_(self.v)\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Final output linear transformation\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        input_shape = q.shape \n",
    "\n",
    "        # Linear & Split heads\n",
    "        q = self.w_q(q).view(input_shape[0], -1, num_heads, d_k) # (B, L, H, d_k)\n",
    "        k = self.w_k(k).view(input_shape[0], -1, num_heads, d_k) \n",
    "        v = self.w_v(v).view(input_shape[0], -1, num_heads, d_k) \n",
    "\n",
    "        # Transpose to (B, H, L, d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # --- Bahdanau Score Calculation ---\n",
    "        # q: (B, H, L_q, 1, d_k)\n",
    "        # k: (B, H, 1, L_k, d_k)\n",
    "        # Broadcast sum -> (B, H, L_q, L_k, d_k)\n",
    "        energy = torch.tanh(q.unsqueeze(3) + k.unsqueeze(2)) \n",
    "        \n",
    "        # Multiply by v and sum last dim -> (B, H, L_q, L_k)\n",
    "        attn_scores = torch.sum(self.v * energy, dim=-1) \n",
    "        # ----------------------------------\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1) \n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1 * self.inf)\n",
    "\n",
    "        attn_distribs = self.attn_softmax(attn_scores)\n",
    "        attn_distribs = self.dropout(attn_distribs)\n",
    "        \n",
    "        attn_values = torch.matmul(attn_distribs, v) \n",
    "        \n",
    "        concat_output = attn_values.transpose(1, 2)\\\n",
    "            .contiguous().view(input_shape[0], -1, d_model)\n",
    "\n",
    "        return self.w_0(concat_output)\n",
    "\n",
    "class LuongMultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inf = 1e9\n",
    "\n",
    "        # Standard projections\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # --- LUONG 'GENERAL' SPECIFIC ---\n",
    "        # Matrix Wa in formula: score(h_t, h_s) = h_t^T * Wa * h_s\n",
    "        # Chúng ta cần một ma trận Wa cho mỗi Head riêng biệt.\n",
    "        # Shape: (num_heads, d_k, d_k)\n",
    "        self.w_a = nn.Parameter(torch.rand(num_heads, d_k, d_k))\n",
    "        nn.init.xavier_uniform_(self.w_a)\n",
    "        # --------------------------------\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_out_rate)\n",
    "        self.attn_softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.w_0 = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        input_shape = q.shape\n",
    "\n",
    "        # 1. Linear & Split heads\n",
    "        q = self.w_q(q).view(input_shape[0], -1, num_heads, d_k) # (B, L_q, H, d_k)\n",
    "        k = self.w_k(k).view(input_shape[0], -1, num_heads, d_k) # (B, L_k, H, d_k)\n",
    "        v = self.w_v(v).view(input_shape[0], -1, num_heads, d_k) \n",
    "\n",
    "        # Transpose => (B, H, L, d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # 2. Luong 'General' Score Calculation: Q * Wa * K^T\n",
    "        # Bước A: Tính Q_weighted = Q * Wa\n",
    "        # Q: (B, H, L_q, d_k)\n",
    "        # Wa: (H, d_k, d_k)\n",
    "        # Chúng ta dùng einsum để nhân ma trận Wa riêng cho từng head\n",
    "        # 'bhld' (q), 'hde' (wa) -> 'bhle' (q_weighted)\n",
    "        q_weighted = torch.einsum('bhld,hde->bhle', q, self.w_a)\n",
    "\n",
    "        # Bước B: Nhân với K^T\n",
    "        attn_scores = torch.matmul(q_weighted, k.transpose(-2, -1)) # (B, H, L_q, L_k)\n",
    "\n",
    "        # Lưu ý: Luong Attention gốc thường không chia cho sqrt(d_k), \n",
    "        # nhưng ta có thể giữ hoặc bỏ tùy ý. Ở đây tôi bỏ scaling để đúng chất Luong General.\n",
    "        \n",
    "        # 3. Masking & Softmax (Standard)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1 * self.inf)\n",
    "\n",
    "        attn_distribs = self.attn_softmax(attn_scores)\n",
    "        attn_distribs = self.dropout(attn_distribs)\n",
    "        \n",
    "        attn_values = torch.matmul(attn_distribs, v)\n",
    "\n",
    "        concat_output = attn_values.transpose(1, 2)\\\n",
    "            .contiguous().view(input_shape[0], -1, d_model)\n",
    "\n",
    "        return self.w_0(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe2d8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.821683Z",
     "iopub.status.busy": "2025-11-29T03:47:45.821164Z",
     "iopub.status.idle": "2025-11-29T03:47:45.832198Z",
     "shell.execute_reply": "2025-11-29T03:47:45.831522Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.016065,
     "end_time": "2025-11-29T03:47:45.833286",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.817221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from tqdm import tqdm\n",
    "from constants import (\n",
    "    SP_DIR,\n",
    "    src_model_prefix,\n",
    "    trg_model_prefix,\n",
    "    seq_len,\n",
    "    pad_id,\n",
    "    sos_id,\n",
    "    eos_id,\n",
    "    learning_rate,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    ckpt_dir,\n",
    "    beam_size,\n",
    "    start_epoch,\n",
    ")\n",
    "from constants import TRAIN_NAME, VALID_NAME\n",
    "from custom_data import get_data_loader, pad_or_truncate\n",
    "from transformer import Transformer\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class Manager:\n",
    "    def __init__(self, is_train=True, ckpt_name=None):\n",
    "        # Load vocabs\n",
    "        print(\"Loading vocabs...\")\n",
    "        self.src_i2w = {}\n",
    "        self.trg_i2w = {}\n",
    "\n",
    "        with open(f\"{SP_DIR}/{src_model_prefix}.vocab\") as f:\n",
    "            lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            word = line.strip().split(\"\\t\")[0]\n",
    "            self.src_i2w[i] = word\n",
    "\n",
    "        with open(f\"{SP_DIR}/{trg_model_prefix}.vocab\") as f:\n",
    "            lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            word = line.strip().split(\"\\t\")[0]\n",
    "            self.trg_i2w[i] = word\n",
    "\n",
    "        print(\n",
    "            f\"The size of src vocab is {len(self.src_i2w)} and that of trg vocab is {len(self.trg_i2w)}.\"\n",
    "        )\n",
    "\n",
    "        # Load Transformer model & Adam optimizer\n",
    "        print(\"Loading Transformer model & Adam optimizer...\")\n",
    "        self.model = Transformer(\n",
    "            src_vocab_size=len(self.src_i2w), trg_vocab_size=len(self.trg_i2w)\n",
    "        )\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Detecting {torch.cuda.device_count()} GPUs. Using DataParallel!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "            self.model_core = self.model.module\n",
    "        else:\n",
    "            self.model_core = self.model\n",
    "        self.model = self.model.to(device)\n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.best_loss = sys.float_info.max\n",
    "        self.start_epoch = start_epoch\n",
    "        print(f\"Starting from epoch {self.start_epoch}\")\n",
    "\n",
    "        if ckpt_name is not None:\n",
    "            assert os.path.exists(f\"{ckpt_dir}/{ckpt_name}\"), (\n",
    "                f\"There is no checkpoint named {ckpt_name}.\"\n",
    "            )\n",
    "\n",
    "            print(\"Loading checkpoint...\")\n",
    "            checkpoint = torch.load(\n",
    "                f\"{ckpt_dir}/{ckpt_name}\", map_location=device, weights_only=False\n",
    "            )\n",
    "            self.model_core.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.optim.load_state_dict(checkpoint[\"optim_state_dict\"])\n",
    "            self.best_loss = checkpoint[\"loss\"]\n",
    "\n",
    "            if \"epoch\" in checkpoint:\n",
    "                self.start_epoch = checkpoint[\"epoch\"]\n",
    "                print(f\"Resuming training from epoch {self.start_epoch}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"No epoch info in checkpoint, starting from epoch {self.start_epoch} (but with loaded weights).\"\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing the model...\")\n",
    "            for p in self.model.parameters():\n",
    "                if p.dim() > 1:\n",
    "                    nn.init.xavier_uniform_(p)\n",
    "\n",
    "        if is_train:\n",
    "            # Load loss function\n",
    "            print(\"Loading loss function...\")\n",
    "            self.criterion = nn.NLLLoss(ignore_index=pad_id)\n",
    "\n",
    "            # Load dataloaders\n",
    "            print(\"Loading dataloaders...\")\n",
    "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
    "            self.valid_loader = get_data_loader(VALID_NAME)\n",
    "\n",
    "        print(\"Setting finished.\")\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Training starts.\")\n",
    "\n",
    "        start_range = self.start_epoch\n",
    "        end_range = self.start_epoch + num_epochs\n",
    "\n",
    "        for epoch in range(start_range, end_range):\n",
    "            self.model.train()\n",
    "\n",
    "            train_losses = []\n",
    "            start_time = datetime.datetime.now()\n",
    "\n",
    "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
    "                src_input, trg_input, trg_output = batch\n",
    "                src_input, trg_input, trg_output = (\n",
    "                    src_input.to(device),\n",
    "                    trg_input.to(device),\n",
    "                    trg_output.to(device),\n",
    "                )\n",
    "\n",
    "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
    "\n",
    "                output = self.model(\n",
    "                    src_input, trg_input, e_mask, d_mask\n",
    "                )  # (B, L, vocab_size)\n",
    "\n",
    "                trg_output_shape = trg_output.shape\n",
    "                self.optim.zero_grad()\n",
    "                loss = self.criterion(\n",
    "                    output.view(-1, self.model_core.trg_vocab_size),\n",
    "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1]),\n",
    "                )\n",
    "\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "\n",
    "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
    "\n",
    "            end_time = datetime.datetime.now()\n",
    "            training_time = end_time - start_time\n",
    "            seconds = training_time.seconds\n",
    "            hours = seconds // 3600\n",
    "            minutes = (seconds % 3600) // 60\n",
    "            seconds = seconds % 60\n",
    "\n",
    "            mean_train_loss = np.mean(train_losses)\n",
    "            print(f\"#################### Epoch: {epoch} ####################\")\n",
    "            print(\n",
    "                f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\"\n",
    "            )\n",
    "\n",
    "            valid_loss, valid_time = self.validation()\n",
    "\n",
    "            if not os.path.exists(ckpt_dir):\n",
    "                os.mkdir(ckpt_dir)\n",
    "\n",
    "            is_best = False\n",
    "            if valid_loss < self.best_loss:\n",
    "                self.best_loss = valid_loss\n",
    "                is_best = True\n",
    "                print(\n",
    "                    f\"***** Epoch {epoch} has best valid loss: {self.best_loss} *****\"\n",
    "                )\n",
    "            state_dict = {\n",
    "                \"model_state_dict\": self.model_core.state_dict(),\n",
    "                \"optim_state_dict\": self.optim.state_dict(),\n",
    "                \"loss\": valid_loss,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"epoch\": epoch,\n",
    "            }\n",
    "            torch.save(state_dict, f\"{ckpt_dir}/ckpt_epoch{epoch}.tar\")\n",
    "            print(f\"Saved checkpoint: ckpt_epoch{epoch}.tar\")\n",
    "\n",
    "            if is_best:\n",
    "                torch.save(state_dict, f\"{ckpt_dir}/best_ckpt.tar\")\n",
    "                print(\"***** Updated best_ckpt.tar *****\")\n",
    "\n",
    "            print(f\"Best valid loss: {self.best_loss}\")\n",
    "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
    "\n",
    "        print(\"Training finished!\")\n",
    "\n",
    "    def validation(self):\n",
    "        print(\"Validation processing...\")\n",
    "        self.model.eval()\n",
    "\n",
    "        valid_losses = []\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
    "                src_input, trg_input, trg_output = batch\n",
    "                src_input, trg_input, trg_output = (\n",
    "                    src_input.to(device),\n",
    "                    trg_input.to(device),\n",
    "                    trg_output.to(device),\n",
    "                )\n",
    "\n",
    "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
    "\n",
    "                output = self.model(\n",
    "                    src_input, trg_input, e_mask, d_mask\n",
    "                )  # (B, L, vocab_size)\n",
    "\n",
    "                trg_output_shape = trg_output.shape\n",
    "                loss = self.criterion(\n",
    "                    output.view(-1, self.model_core.trg_vocab_size),\n",
    "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1]),\n",
    "                )\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "        validation_time = end_time - start_time\n",
    "        seconds = validation_time.seconds\n",
    "        hours = seconds // 3600\n",
    "        minutes = (seconds % 3600) // 60\n",
    "        seconds = seconds % 60\n",
    "\n",
    "        mean_valid_loss = np.mean(valid_losses)\n",
    "\n",
    "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
    "\n",
    "    def inference(self, input_sentence, method):\n",
    "        print(\"Inference starts.\")\n",
    "        self.model.eval()\n",
    "\n",
    "        print(\"Loading sentencepiece tokenizer...\")\n",
    "        src_sp = spm.SentencePieceProcessor()\n",
    "        trg_sp = spm.SentencePieceProcessor()\n",
    "        src_sp.Load(f\"{SP_DIR}/{src_model_prefix}.model\")\n",
    "        trg_sp.Load(f\"{SP_DIR}/{trg_model_prefix}.model\")\n",
    "\n",
    "        print(\"Preprocessing input sentence...\")\n",
    "        tokenized = src_sp.EncodeAsIds(input_sentence)\n",
    "        src_data = (\n",
    "            torch.LongTensor(pad_or_truncate(tokenized)).unsqueeze(0).to(device)\n",
    "        )  # (1, L)\n",
    "        e_mask = (src_data != pad_id).unsqueeze(1).to(device)  # (1, 1, L)\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        print(\"Encoding input sentence...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            src_data = self.model_core.src_embedding(src_data)\n",
    "            src_data = self.model_core.positional_encoder(src_data)\n",
    "            e_output = self.model_core.encoder(src_data, e_mask)  # (1, L, d_model)\n",
    "\n",
    "            if method == \"greedy\":\n",
    "                print(\"Greedy decoding selected.\")\n",
    "                result = self.greedy_search(e_output, e_mask, trg_sp)\n",
    "            elif method == \"beam\":\n",
    "                print(\"Beam search selected.\")\n",
    "                result = self.beam_search(e_output, e_mask, trg_sp)\n",
    "\n",
    "        end_time = datetime.datetime.now()\n",
    "\n",
    "        total_inference_time = end_time - start_time\n",
    "        seconds = total_inference_time.seconds\n",
    "        minutes = seconds // 60\n",
    "        seconds = seconds % 60\n",
    "\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        print(\n",
    "            f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\"\n",
    "        )\n",
    "\n",
    "        return result\n",
    "\n",
    "    def greedy_search(self, e_output, e_mask, trg_sp):\n",
    "        last_words = torch.LongTensor([pad_id] * seq_len).to(device)  # (L)\n",
    "        last_words[0] = sos_id  # (L)\n",
    "        cur_len = 1\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            d_mask = (\n",
    "                (last_words.unsqueeze(0) != pad_id).unsqueeze(1).to(device)\n",
    "            )  # (1, 1, L)\n",
    "            nopeak_mask = torch.ones([1, seq_len, seq_len], dtype=torch.bool).to(\n",
    "                device\n",
    "            )  # (1, L, L)\n",
    "            nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L) to triangular shape\n",
    "            d_mask = d_mask & nopeak_mask  # (1, L, L) padding false\n",
    "\n",
    "            trg_embedded = self.model_core.trg_embedding(last_words.unsqueeze(0))\n",
    "            trg_positional_encoded = self.model_core.positional_encoder(trg_embedded)\n",
    "            decoder_output = self.model_core.decoder(\n",
    "                trg_positional_encoded, e_output, e_mask, d_mask\n",
    "            )  # (1, L, d_model)\n",
    "\n",
    "            output = self.model_core.softmax(\n",
    "                self.model_core.output_linear(decoder_output)\n",
    "            )  # (1, L, trg_vocab_size)\n",
    "\n",
    "            output = torch.argmax(output, dim=-1)  # (1, L)\n",
    "            last_word_id = output[0][i].item()\n",
    "\n",
    "            if i < seq_len - 1:\n",
    "                last_words[i + 1] = last_word_id\n",
    "                cur_len += 1\n",
    "\n",
    "            if last_word_id == eos_id:\n",
    "                break\n",
    "\n",
    "        if last_words[-1].item() == pad_id:\n",
    "            decoded_output = last_words[1:cur_len].tolist()\n",
    "        else:\n",
    "            decoded_output = last_words[1:].tolist()\n",
    "        decoded_output = trg_sp.decode_ids(decoded_output)\n",
    "\n",
    "        return decoded_output\n",
    "\n",
    "    def beam_search(self, e_output, e_mask, trg_sp, beam_size=beam_size, alpha=0.7):\n",
    "        \"\"\"\n",
    "        Beam search implemented correctly.\n",
    "        - e_output: encoder outputs (1, L_enc, d_model)\n",
    "        - e_mask: encoder mask (1, 1, L_enc)\n",
    "        - trg_sp: SentencePiece processor for decoding ids->text\n",
    "        - beam_size: beam width\n",
    "        - alpha: length normalization hyperparameter (common default ~0.7)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Each hypothesis: (tokens_list, cumulative_logprob, is_finished)\n",
    "        # Initialize with single hypothesis [SOS]\n",
    "        hypotheses = [([sos_id], 0.0, False)]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            all_candidates = []\n",
    "\n",
    "            # If all hypotheses are finished, we can stop early\n",
    "            if all(h[2] for h in hypotheses):\n",
    "                break\n",
    "\n",
    "            # Build batch of decoder inputs: for every hypothesis that is not finished,\n",
    "            # we'll expand with top-k next token candidates. For finished hypos, keep them as-is.\n",
    "            # We will run decoder on the batch of candidate sequences to get log-probs.\n",
    "            for h_idx, (tokens, logp, finished) in enumerate(hypotheses):\n",
    "                if finished:\n",
    "                    # keep finished hypothesis as a candidate (carry over)\n",
    "                    all_candidates.append((tokens, logp, True))\n",
    "                else:\n",
    "                    # We will expand this hypothesis; but first create its current input (padded)\n",
    "                    # We'll ask the model for top-k next tokens; to do that efficiently we will\n",
    "                    # create candidate inputs later.\n",
    "                    # For now just note we will expand this hypothesis.\n",
    "                    # We'll create the actual candidate inputs after we determine top-k per hypo.\n",
    "                    pass\n",
    "\n",
    "            # To get top-k for each hypothesis we need model output at position t given its tokens.\n",
    "            # We'll create a batch of current hypotheses (one per non-finished hypo), run decoder,\n",
    "            # and extract log-probs at time step t, then pick top-k per hypothesis.\n",
    "            alive_hypos = [(idx, h) for idx, h in enumerate(hypotheses) if not h[2]]\n",
    "            if len(alive_hypos) == 0:\n",
    "                break\n",
    "\n",
    "            # Prepare batch input: for each alive hypo, create padded tensor (seq_len) with its tokens\n",
    "            batch_inputs = []\n",
    "            hypo_map = []  # map from batch row -> hypothesis index\n",
    "            for h_idx, (tokens, logp, finished) in alive_hypos:\n",
    "                seq = tokens + [pad_id] * (seq_len - len(tokens))\n",
    "                batch_inputs.append(seq)\n",
    "                hypo_map.append(h_idx)\n",
    "\n",
    "            batch_inputs = torch.LongTensor(batch_inputs).to(\n",
    "                device\n",
    "            )  # (B_alive, seq_len)\n",
    "\n",
    "            # Create decoder mask for this batch\n",
    "            d_mask = (\n",
    "                (batch_inputs != pad_id).unsqueeze(1).to(device)\n",
    "            )  # (B_alive, 1, seq_len)\n",
    "            nopeak = torch.tril(\n",
    "                torch.ones((1, seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "            )\n",
    "            d_mask = d_mask & nopeak  # (B_alive, seq_len, seq_len) broadcasted\n",
    "\n",
    "            # Run decoder for this batch (one forward)\n",
    "            with torch.no_grad():\n",
    "                trg_emb = self.model_core.trg_embedding(\n",
    "                    batch_inputs\n",
    "                )  # (B_alive, L, d_model)\n",
    "                trg_emb = self.model_core.positional_encoder(\n",
    "                    trg_emb\n",
    "                )  # (B_alive, L, d_model)\n",
    "                dec_out = self.model_core.decoder(\n",
    "                    trg_emb,\n",
    "                    e_output.repeat(len(batch_inputs), 1, 1)\n",
    "                    if e_output.size(0) == 1\n",
    "                    else e_output,\n",
    "                    e_mask.repeat(len(batch_inputs), 1, 1)\n",
    "                    if e_mask.size(0) == 1\n",
    "                    else e_mask,\n",
    "                    d_mask,\n",
    "                )  # (B_alive, L, d_model)\n",
    "                # Get log-probs (use model's output_linear then log_softmax to be safe)\n",
    "                logits = self.model_core.output_linear(dec_out)  # (B_alive, L, V)\n",
    "                log_probs = F.log_softmax(logits, dim=-1)  # (B_alive, L, V)\n",
    "\n",
    "            # For each alive hypothesis, get top-k tokens at time step t\n",
    "            B_alive = log_probs.size(0)\n",
    "            V = log_probs.size(-1)\n",
    "            topk = min(beam_size, V)\n",
    "\n",
    "            for i in range(B_alive):\n",
    "                hypo_idx = hypo_map[i]\n",
    "                tokens, curr_logp, _ = hypotheses[hypo_idx]\n",
    "                # get log-prob vector at time t\n",
    "                logp_t = log_probs[i, t]  # (V,)\n",
    "                top_vals, top_idx = torch.topk(logp_t, k=topk)  # both tensors\n",
    "                top_vals = top_vals.cpu().tolist()\n",
    "                top_idx = top_idx.cpu().tolist()\n",
    "                for k_idx, token_id in enumerate(top_idx):\n",
    "                    new_tokens = tokens + [token_id]\n",
    "                    new_logp = curr_logp + top_vals[k_idx]  # cumulative log-prob\n",
    "                    finished = token_id == eos_id\n",
    "                    all_candidates.append((new_tokens, new_logp, finished))\n",
    "\n",
    "            # Also include previous finished hypotheses (they were added earlier)\n",
    "\n",
    "            # Now select top `beam_size` candidates among all_candidates by cumulative log-prob\n",
    "            # Note: do NOT normalize length here (we keep cumulative log-prob for beam propagation).\n",
    "            all_candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "            hypotheses = all_candidates[:beam_size]\n",
    "\n",
    "            # If number of hypotheses < beam_size (possible if many finished), pad by carrying best finished\n",
    "            # (not strictly necessary)\n",
    "\n",
    "        # After finishing (either reached seq_len or all finished), choose best hypothesis with length normalization\n",
    "        # Apply length normalization score = logp / (len(tokens) ** alpha)\n",
    "        final_scores = []\n",
    "        for tokens, logp, finished in hypotheses:\n",
    "            length = len(tokens) - 1  # exclude SOS for length\n",
    "            if length <= 0:\n",
    "                length = 1.0\n",
    "            score = logp / (length**alpha)\n",
    "            final_scores.append((score, tokens, finished))\n",
    "\n",
    "        final_scores = sorted(final_scores, key=lambda x: x[0], reverse=True)\n",
    "        best_tokens = final_scores[0][1]\n",
    "\n",
    "        # Remove leading SOS and trailing EOS if present\n",
    "        if best_tokens and best_tokens[0] == sos_id:\n",
    "            best_tokens = best_tokens[1:]\n",
    "        if best_tokens and best_tokens[-1] == eos_id:\n",
    "            best_tokens = best_tokens[:-1]\n",
    "\n",
    "        return trg_sp.decode_ids(best_tokens)\n",
    "\n",
    "    def make_mask(self, src_input, trg_input):\n",
    "        e_mask = (src_input != pad_id).unsqueeze(1).to(device)\n",
    "        d_mask = (trg_input != pad_id).unsqueeze(1).to(device)\n",
    "\n",
    "        nopeak_mask = torch.tril(\n",
    "            torch.ones((1, seq_len, seq_len), dtype=torch.bool, device=device)\n",
    "        )  # (1, L, L) to triangular shape\n",
    "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
    "\n",
    "        return e_mask, d_mask\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", required=True, help=\"train or inference?\")\n",
    "    parser.add_argument(\"--ckpt_name\", required=False, help=\"best checkpoint file\")\n",
    "    parser.add_argument(\n",
    "        \"--input\", type=str, required=False, help=\"input sentence when inferencing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decode\", type=str, required=False, default=\"greedy\", help=\"greedy or beam?\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        if args.ckpt_name is not None:\n",
    "            manager = Manager(is_train=True, ckpt_name=args.ckpt_name)\n",
    "        else:\n",
    "            manager = Manager(is_train=True)\n",
    "\n",
    "        manager.train()\n",
    "    elif args.mode == \"inference\":\n",
    "        assert args.ckpt_name is not None, (\n",
    "            \"Please specify the model file name you want to use.\"\n",
    "        )\n",
    "        assert args.input is not None, \"Please specify the input sentence to translate.\"\n",
    "        assert args.decode == \"greedy\" or args.decode == \"beam\", (\n",
    "            \"Please specify correct decoding method, either 'greedy' or 'beam'.\"\n",
    "        )\n",
    "\n",
    "        manager = Manager(is_train=False, ckpt_name=args.ckpt_name)\n",
    "        manager.inference(args.input, args.decode)\n",
    "\n",
    "    else:\n",
    "        print(\"Please specify mode argument either with 'train' or 'inference'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66f782c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.840364Z",
     "iopub.status.busy": "2025-11-29T03:47:45.839908Z",
     "iopub.status.idle": "2025-11-29T03:47:45.844616Z",
     "shell.execute_reply": "2025-11-29T03:47:45.843922Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.009421,
     "end_time": "2025-11-29T03:47:45.845787",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.836366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sentencepiece_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sentencepiece_train.py\n",
    "from constants import DATA_DIR, SP_DIR, SRC_RAW_DATA_NAME, TRG_RAW_DATA_NAME\n",
    "from constants import src_model_prefix, trg_model_prefix, pad_id, sos_id, eos_id, unk_id, sp_vocab_size, character_coverage, model_type\n",
    "from constants import SRC_DIR, TRG_DIR, TRAIN_NAME, VALID_NAME\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "train_frac = 0.8\n",
    "\n",
    "def train_sp(is_src=True):\n",
    "    template = \"--input={} \\\n",
    "                --pad_id={} \\\n",
    "                --bos_id={} \\\n",
    "                --eos_id={} \\\n",
    "                --unk_id={} \\\n",
    "                --model_prefix={} \\\n",
    "                --vocab_size={} \\\n",
    "                --character_coverage={} \\\n",
    "                --model_type={}\"\n",
    "\n",
    "    if is_src:\n",
    "        this_input_file = f\"{DATA_DIR}/{SRC_RAW_DATA_NAME}\"\n",
    "        this_model_prefix = f\"{SP_DIR}/{src_model_prefix}\"\n",
    "    else:\n",
    "        this_input_file = f\"{DATA_DIR}/{TRG_RAW_DATA_NAME}\"\n",
    "        this_model_prefix = f\"{SP_DIR}/{trg_model_prefix}\"\n",
    "\n",
    "    config = template.format(this_input_file,\n",
    "                            pad_id,\n",
    "                            sos_id,\n",
    "                            eos_id,\n",
    "                            unk_id,\n",
    "                            this_model_prefix,\n",
    "                            sp_vocab_size,\n",
    "                            character_coverage,\n",
    "                            model_type)\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    if not os.path.isdir(SP_DIR):\n",
    "        os.mkdir(SP_DIR)\n",
    "\n",
    "    print(spm)\n",
    "    spm.SentencePieceTrainer.Train(config)\n",
    "    \n",
    "    \n",
    "def split_data(raw_data_name, data_dir):\n",
    "    with open(f\"{DATA_DIR}/{raw_data_name}\") as f:\n",
    "        lines = f.readlines()    \n",
    "    \n",
    "    print(\"Splitting data...\")\n",
    "    \n",
    "    train_lines = lines[:int(train_frac * len(lines))]\n",
    "    valid_lines = lines[int(train_frac * len(lines)):]\n",
    "    \n",
    "    if not os.path.isdir(f\"{DATA_DIR}/{data_dir}\"):\n",
    "        os.mkdir(f\"{DATA_DIR}/{data_dir}\")\n",
    "    \n",
    "    with open(f\"{DATA_DIR}/{data_dir}/{TRAIN_NAME}\", 'w') as f:\n",
    "        for line in tqdm(train_lines):\n",
    "            f.write(line.strip() + '\\n')\n",
    "            \n",
    "    with open(f\"{DATA_DIR}/{data_dir}/{VALID_NAME}\", 'w') as f:\n",
    "        for line in tqdm(valid_lines):\n",
    "            f.write(line.strip() + '\\n')\n",
    "            \n",
    "    print(f\"Train/Validation data saved in {DATA_DIR}/{data_dir}.\")\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    train_sp(is_src=True)\n",
    "    train_sp(is_src=False)\n",
    "    split_data(SRC_RAW_DATA_NAME, SRC_DIR)\n",
    "    split_data(TRG_RAW_DATA_NAME, TRG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0b709b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.853074Z",
     "iopub.status.busy": "2025-11-29T03:47:45.852874Z",
     "iopub.status.idle": "2025-11-29T03:47:45.857432Z",
     "shell.execute_reply": "2025-11-29T03:47:45.856790Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.009552,
     "end_time": "2025-11-29T03:47:45.858440",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.848888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformer.py\n",
    "from torch import nn\n",
    "from constants import d_model, num_layers\n",
    "from layers import EncoderLayer, DecoderLayer, PositionalEncoder, LayerNormalization\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
    "        super().__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "        self.src_embedding = nn.Embedding(self.src_vocab_size, d_model)\n",
    "        self.trg_embedding = nn.Embedding(self.trg_vocab_size, d_model)\n",
    "        self.positional_encoder = PositionalEncoder()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.output_linear = nn.Linear(d_model, self.trg_vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
    "        src_input = self.src_embedding(src_input) # (B, L) => (B, L, d_model)\n",
    "        trg_input = self.trg_embedding(trg_input) # (B, L) => (B, L, d_model)\n",
    "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
    "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
    "\n",
    "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
    "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
    "\n",
    "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for i in range(num_layers)])\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, e_mask):\n",
    "        for i in range(num_layers):\n",
    "            x = self.layers[i](x, e_mask)\n",
    "\n",
    "        return self.layer_norm(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for i in range(num_layers)])\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, e_output, e_mask, d_mask):\n",
    "        for i in range(num_layers):\n",
    "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
    "\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e222dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T03:47:45.865651Z",
     "iopub.status.busy": "2025-11-29T03:47:45.865221Z",
     "iopub.status.idle": "2025-11-29T11:30:00.804228Z",
     "shell.execute_reply": "2025-11-29T11:30:00.803200Z"
    },
    "papermill": {
     "duration": 27734.944408,
     "end_time": "2025-11-29T11:30:00.805910",
     "exception": false,
     "start_time": "2025-11-29T03:47:45.861502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabs...\r\n",
      "The size of src vocab is 16000 and that of trg vocab is 16000.\r\n",
      "Loading Transformer model & Adam optimizer...\r\n",
      "Detecting 2 GPUs. Using DataParallel!\r\n",
      "Starting from epoch 1\r\n",
      "Initializing the model...\r\n",
      "Loading loss function...\r\n",
      "Loading dataloaders...\r\n",
      "Getting source/target train.txt...\r\n",
      "Tokenizing & Padding src data...\r\n",
      "100%|████████████████████████████████| 560000/560000 [00:13<00:00, 41150.81it/s]\r\n",
      "The shape of src data: (560000, 128)\r\n",
      "Tokenizing & Padding trg data...\r\n",
      "100%|████████████████████████████████| 560000/560000 [00:18<00:00, 30012.17it/s]\r\n",
      "The shape of input trg data: (560000, 128)\r\n",
      "The shape of output trg data: (560000, 128)\r\n",
      "Getting source/target valid.txt...\r\n",
      "Tokenizing & Padding src data...\r\n",
      "100%|████████████████████████████████| 140000/140000 [00:03<00:00, 45802.70it/s]\r\n",
      "The shape of src data: (140000, 128)\r\n",
      "Tokenizing & Padding trg data...\r\n",
      "100%|████████████████████████████████| 140000/140000 [00:04<00:00, 33029.04it/s]\r\n",
      "The shape of input trg data: (140000, 128)\r\n",
      "The shape of output trg data: (140000, 128)\r\n",
      "Setting finished.\r\n",
      "Training starts.\r\n",
      "8750it [1:23:59,  1.74it/s]\r\n",
      "#################### Epoch: 1 ####################\r\n",
      "Train loss: 3.3241005281175884 || One epoch training time: 1hrs 23mins 59secs\r\n",
      "Validation processing...\r\n",
      "2188it [08:02,  4.54it/s]\r\n",
      "***** Epoch 1 has best valid loss: 2.310589930039219 *****\r\n",
      "Saved checkpoint: ckpt_epoch1.tar\r\n",
      "***** Updated best_ckpt.tar *****\r\n",
      "Best valid loss: 2.310589930039219\r\n",
      "Valid loss: 2.310589930039219 || One epoch training time: 0hrs 8mins 2secs\r\n",
      "8750it [1:24:02,  1.74it/s]\r\n",
      "#################### Epoch: 2 ####################\r\n",
      "Train loss: 2.1687731866155353 || One epoch training time: 1hrs 24mins 2secs\r\n",
      "Validation processing...\r\n",
      "2188it [08:01,  4.54it/s]\r\n",
      "***** Epoch 2 has best valid loss: 1.9687064150024811 *****\r\n",
      "Saved checkpoint: ckpt_epoch2.tar\r\n",
      "***** Updated best_ckpt.tar *****\r\n",
      "Best valid loss: 1.9687064150024811\r\n",
      "Valid loss: 1.9687064150024811 || One epoch training time: 0hrs 8mins 1secs\r\n",
      "8750it [1:24:01,  1.74it/s]\r\n",
      "#################### Epoch: 3 ####################\r\n",
      "Train loss: 1.8916166332108635 || One epoch training time: 1hrs 24mins 1secs\r\n",
      "Validation processing...\r\n",
      "2188it [08:00,  4.55it/s]\r\n",
      "***** Epoch 3 has best valid loss: 1.8109472087793856 *****\r\n",
      "Saved checkpoint: ckpt_epoch3.tar\r\n",
      "***** Updated best_ckpt.tar *****\r\n",
      "Best valid loss: 1.8109472087793856\r\n",
      "Valid loss: 1.8109472087793856 || One epoch training time: 0hrs 8mins 0secs\r\n",
      "8750it [1:24:00,  1.74it/s]\r\n",
      "#################### Epoch: 4 ####################\r\n",
      "Train loss: 1.734857055555071 || One epoch training time: 1hrs 24mins 0secs\r\n",
      "Validation processing...\r\n",
      "2188it [08:01,  4.55it/s]\r\n",
      "***** Epoch 4 has best valid loss: 1.7208934296092777 *****\r\n",
      "Saved checkpoint: ckpt_epoch4.tar\r\n",
      "***** Updated best_ckpt.tar *****\r\n",
      "Best valid loss: 1.7208934296092777\r\n",
      "Valid loss: 1.7208934296092777 || One epoch training time: 0hrs 8mins 1secs\r\n",
      "8750it [1:24:02,  1.74it/s]\r\n",
      "#################### Epoch: 5 ####################\r\n",
      "Train loss: 1.6253795264516557 || One epoch training time: 1hrs 24mins 2secs\r\n",
      "Validation processing...\r\n",
      "2188it [08:01,  4.54it/s]\r\n",
      "***** Epoch 5 has best valid loss: 1.6681830819280754 *****\r\n",
      "Saved checkpoint: ckpt_epoch5.tar\r\n",
      "***** Updated best_ckpt.tar *****\r\n",
      "Best valid loss: 1.6681830819280754\r\n",
      "Valid loss: 1.6681830819280754 || One epoch training time: 0hrs 8mins 1secs\r\n",
      "Training finished!\r\n"
     ]
    }
   ],
   "source": [
    "!python main.py --mode train"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 281718226,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27752.230663,
   "end_time": "2025-11-29T11:30:03.905509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T03:47:31.674846",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
